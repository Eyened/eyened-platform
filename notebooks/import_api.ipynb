{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import base64\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "from utils import download_and_extract_dataset\n",
    "\n",
    "# API configuration\n",
    "API_BASE_URL = \"http://eyened-gpu:2222/api\"  # Adjust this to your server URL\n",
    "API_USERNAME = \"admin\"  # Replace with your API username\n",
    "API_PASSWORD = \"CHANGE_ME\"  # Replace with your API password\n",
    "\n",
    "# Create authentication header\n",
    "auth_str = f\"{API_USERNAME}:{API_PASSWORD}\"\n",
    "auth_bytes = auth_str.encode('ascii')\n",
    "base64_auth = base64.b64encode(auth_bytes).decode('ascii')\n",
    "headers = {\n",
    "    \"Authorization\": f\"Basic {base64_auth}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www5.cs.fau.de/fileadmin/research/datasets/fundus-images/all.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72.8M/72.8M [00:00<00:00, 84.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting to hrfav...\n",
      "Download and extraction complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hrfav'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download HRF dataset\n",
    "download_and_extract_dataset('https://www5.cs.fau.de/fileadmin/research/datasets/fundus-images/all.zip', 'hrfav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up project information\n",
    "project_name = \"HRFAV Fundus Dataset\"\n",
    "extract_dir = Path(\"./hrfav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images.\n"
     ]
    }
   ],
   "source": [
    "# Get all image paths\n",
    "images_dir = extract_dir / \"images\"\n",
    "image_paths = list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.JPG\"))\n",
    "\n",
    "print(f\"Found {len(image_paths)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data structure with 11 patients.\n",
      "First patient has 4 images.\n"
     ]
    }
   ],
   "source": [
    "# The dataset has images named as g0001.jpg, g0002.jpg, etc.\n",
    "# Let's group images together in batches to demonstrate the hierarchy\n",
    "\n",
    "# Create the data structure for the Importer\n",
    "# Even though in this dataset each image is of a different patient\n",
    "# We'll create one \"patient\" for every 10 images as an example\n",
    "data = []\n",
    "batch_size = 4\n",
    "\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_images = image_paths[i : i + batch_size]\n",
    "\n",
    "    # Create a patient entry (without identifier, will be auto-generated)\n",
    "    patient_item = {\n",
    "        \"patient_identifier\": f\"Patient_{i // batch_size + 1}\",\n",
    "        \"studies\": [\n",
    "            {\n",
    "                \"study_date\": date.today().isoformat(),  # Convert date to ISO format string for JSON\n",
    "                \"series\": [\n",
    "                    {\n",
    "                        \"images\": [\n",
    "                            {\n",
    "                                \"image\": str(img_path.absolute()),\n",
    "                                \"props\": {\n",
    "                                    \"OldPath\": img_path.stem,\n",
    "                                    \"Laterality\": \"R\" if i % 2 == 0 else \"L\"\n",
    "                                    # important! laterality is currently required by the viewer\n",
    "                                },\n",
    "                            }\n",
    "                            for i, img_path in enumerate(batch_images)\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    data.append(patient_item)\n",
    "\n",
    "print(f\"Created data structure with {len(data)} patients.\")\n",
    "print(f\"First patient has {len(data[0]['studies'][0]['series'][0]['images'])} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summaries\n",
    "\n",
    "The summary endpoint will return a summary of what would be imported without writing anything to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Patient with identifier 'Patient_1' not found and create_patients=False\n"
     ]
    }
   ],
   "source": [
    "# Create the request payload with default settings\n",
    "# Because none of the objects exist\n",
    "# We run into an error if we run with default settings: create_series=True, create_studies=False, create_patients=False\n",
    "payload = {\n",
    "    \"data\": data,\n",
    "    \"options\": {\n",
    "        \"project_name\": project_name,\n",
    "        \"run_ai_models\": True,\n",
    "        \"generate_thumbnails\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call the summary endpoint\n",
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/import/summary\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    if result[\"success\"]:\n",
    "        print(\"Summary generated successfully:\")\n",
    "        print(json.dumps(result[\"data\"], indent=2))\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        if result.get(\"stack_trace\"):\n",
    "            print(\"Stack trace:\")\n",
    "            print(result[\"stack_trace\"])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary generated successfully:\n",
      "  Entity  Total  New  Existing  New_Percentage  Existing_Percentage\n",
      "Patients     11   11         0           100.0                  0.0\n",
      " Studies     11   11         0           100.0                  0.0\n",
      "  Series     11   11         0           100.0                  0.0\n",
      "  Images     44   44         0           100.0                  0.0\n",
      "\n",
      "Populated Patients Columns:\n",
      "           Column  Populated  Percentage\n",
      "PatientIdentifier         11       100.0\n",
      "\n",
      "Populated Studies Columns:\n",
      "   Column  Populated  Percentage\n",
      "StudyDate         11       100.0\n",
      "\n",
      "Populated Images Columns:\n",
      "           Column  Populated  Percentage\n",
      "DatasetIdentifier         44       100.0\n",
      "       Laterality         44       100.0\n",
      "          OldPath         44       100.0\n"
     ]
    }
   ],
   "source": [
    "# We need to specify create_patients=True, create_studies=True\n",
    "# since we're not providing identifiers\n",
    "payload = {\n",
    "    \"data\": data,\n",
    "    \"options\": {\n",
    "        \"project_name\": project_name,\n",
    "        \"create_patients\": True,\n",
    "        \"create_studies\": True,\n",
    "        \"run_ai_models\": False,\n",
    "        \"generate_thumbnails\": True,\n",
    "        \"include_stack_trace\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call the summary endpoint\n",
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/import/summary\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    if result[\"success\"]:\n",
    "        print(\"Summary generated successfully:\")\n",
    "        summary = result[\"data\"]\n",
    "        print(pd.DataFrame(summary[\"general_stats\"]).to_string(index=False))\n",
    "        for name, stats in summary[\"column_stats\"].items():\n",
    "            if stats:\n",
    "                print(f\"\\nPopulated {name.capitalize()} Columns:\")\n",
    "                df_columns = pd.DataFrame(stats)\n",
    "                # Format percentage for display\n",
    "                print(df_columns.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        if result.get(\"stack_trace\"):\n",
    "            print(\"Stack trace:\")\n",
    "            print(result[\"stack_trace\"])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Once satisfied with the summary, commit the import to the database with the exec endpoint\n",
    "There might still be Exceptions generated during import, in which case nothing will change in the DB and no files will be written.\n",
    "\n",
    "Post-insertion scripts will run after insertion for non-essential steps such as:\n",
    "\n",
    "- Thumbnail generation (highly recommended to run)\n",
    "- Running image preprocessing scripts (eg. CFI bounds detection)\n",
    "- Running AI models which populate DB columns\n",
    "- Hashing the files for error and duplicate checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import completed successfully:\n",
      "{\n",
      "  \"project_name\": \"FAU Fundus Dataset\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Call the exec endpoint with the same payload\n",
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/import/exec\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    if result[\"success\"]:\n",
    "        print(\"Import completed successfully:\")\n",
    "        print(json.dumps(result[\"data\"], indent=2))\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        if result.get(\"stack_trace\"):\n",
    "            print(\"Stack trace:\")\n",
    "            print(result[\"stack_trace\"])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Results\n",
    "\n",
    "The project is now in the DB. We can use the viewer to inspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating existing projects\n",
    "\n",
    "Images can be inserted into existing projects, patients, studies and series by passing an existing project name, patient_identifier, study_date or series_id in the input structure. These will be matched to database entities (taking into account their nested structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: importer_copy_path must be set when copy_files is True\n"
     ]
    }
   ],
   "source": [
    "# To simulate inserting into an existing project, we'll insert the same data again\n",
    "# Use create_patients=False, create_studies=False, create_series=False when inserting into existing objects to ensure that no new objects will be created\n",
    "# copy_files=True will copy the files to a configurable directory\n",
    "payload = {\n",
    "    \"data\": data,\n",
    "    \"options\": {\n",
    "        \"project_name\": project_name,\n",
    "        \"run_ai_models\": True,\n",
    "        \"generate_thumbnails\": True,\n",
    "        \"copy_files\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# First, get a summary of what would be imported\n",
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/import/summary\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    if result[\"success\"]:\n",
    "        print(\"Summary generated successfully:\")\n",
    "        print(json.dumps(result[\"data\"], indent=2))\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        if result.get(\"stack_trace\"):\n",
    "            print(\"Stack trace:\")\n",
    "            print(result[\"stack_trace\"])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now execute the import\n",
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/import/exec\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    if result[\"success\"]:\n",
    "        print(\"Import completed successfully:\")\n",
    "        print(json.dumps(result[\"data\"], indent=2))\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        if result.get(\"stack_trace\"):\n",
    "            print(\"Stack trace:\")\n",
    "            print(result[\"stack_trace\"])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the updated project data\n",
    "response = requests.get(\n",
    "    f\"{API_BASE_URL}/projects/{project_name}\",\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    project_data = response.json()\n",
    "    print(f\"Project: {project_data['name']}\")\n",
    "    print(f\"ID: {project_data['id']}\")\n",
    "    \n",
    "    # Get the images for this project\n",
    "    response = requests.get(\n",
    "        f\"{API_BASE_URL}/projects/{project_data['id']}/images\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        images = response.json()\n",
    "        print(f\"\\nTotal images: {len(images)}\")\n",
    "        \n",
    "        # Display the images\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(images)\n",
    "        print(\"\\nImages:\")\n",
    "        display(df)\n",
    "    else:\n",
    "        print(f\"Error getting images: {response.status_code}\")\n",
    "        print(response.text)\n",
    "else:\n",
    "    print(f\"Error getting project: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viewer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
