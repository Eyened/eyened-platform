{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRFAV Fundus Dataset - Single Image Import\n",
    "\n",
    "This notebook demonstrates how to import images one by one using the new `/import/image` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import base64\n",
    "from datetime import date\n",
    "from utils import download_and_extract_dataset\n",
    "\n",
    "# API configuration\n",
    "IMAGES_BASEPATH=\"/home/jose/viewer_file/images\" # the same set in .env\n",
    "API_BASE_URL = \"http://eyened-gpu:2222/api\"  # Adjust this to your server URL\n",
    "API_USERNAME = \"admin\"  # Replace with your API username\n",
    "API_PASSWORD = \"CHANGE_ME\"  # Replace with your API password\n",
    "\n",
    "# Create authentication header\n",
    "auth_str = f\"{API_USERNAME}:{API_PASSWORD}\"\n",
    "auth_bytes = auth_str.encode('ascii')\n",
    "base64_auth = base64.b64encode(auth_bytes).decode('ascii')\n",
    "headers = {\n",
    "    \"Authorization\": f\"Basic {base64_auth}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www5.cs.fau.de/fileadmin/research/datasets/fundus-images/all.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72.8M/72.8M [00:00<00:00, 89.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting to /home/jose/viewer_file/images/hrfav...\n",
      "Download and extraction complete!\n"
     ]
    }
   ],
   "source": [
    "# Download HRF dataset if not already downloaded\n",
    "extract_dir = Path(IMAGES_BASEPATH) / \"hrfav\"\n",
    "if not extract_dir.exists():\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    download_and_extract_dataset('https://www5.cs.fau.de/fileadmin/research/datasets/fundus-images/all.zip', extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up project information\n",
    "project_name = \"HRFAV Fundus Dataset - One by One\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images.\n"
     ]
    }
   ],
   "source": [
    "# Get all image paths\n",
    "images_dir = extract_dir / \"images\"\n",
    "image_paths = list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.JPG\"))\n",
    "\n",
    "print(f\"Found {len(image_paths)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Images One by One\n",
    "\n",
    "Unlike the bulk import approach, we'll now import each image individually using the `/import/image` endpoint.\n",
    "\n",
    "First, we'll create a project structure where each image gets its own patient. Each image will be assigned a patient ID based on the image name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define import options - we need to create patients, studies, and project\n",
    "import_options = {\n",
    "    \"create_patients\": True,\n",
    "    \"create_studies\": True, \n",
    "    \"create_series\": True,\n",
    "    \"create_project\": True,\n",
    "    \"include_stack_trace\": True\n",
    "}\n",
    "\n",
    "# For tracking results\n",
    "import_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206e60cf5ae84b35906070390f023911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Importing images:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import each image individually\n",
    "for idx, img_path in enumerate(tqdm(image_paths, desc=\"Importing images\")):\n",
    "    # Create unique patient and series identifiers based on the image name\n",
    "    patient_id = f\"Patient_{idx//2}\"\n",
    "    \n",
    "    # Create image data structure for a single image\n",
    "    image_data = {\n",
    "        \"project_name\": project_name,\n",
    "        \"patient_identifier\": patient_id,\n",
    "        \"study_date\": date.today().isoformat(),\n",
    "        \"image\": str(img_path.absolute()),\n",
    "        \"image_props\": {\n",
    "            \"OldPath\": img_path.stem,\n",
    "            \"Laterality\": \"R\" if idx % 2 == 0 else \"L\"  # Alternating laterality for demo\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the payload\n",
    "    payload = {\n",
    "        \"data\": image_data,\n",
    "        \"options\": import_options\n",
    "    }\n",
    "    \n",
    "    # Call the image import endpoint\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE_URL}/import/image\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    result = response.json()\n",
    "    import_results.append(result)\n",
    "    \n",
    "    # If an error occurred, print it but continue with the next image\n",
    "    if not result[\"success\"]:\n",
    "        print(f\"Error importing {img_path.name}: {result['error']}\")\n",
    "        print(result['stack_trace'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 45\n",
      "Successfully imported: 45\n",
      "Failed imports: 0\n"
     ]
    }
   ],
   "source": [
    "# Summarize the import results\n",
    "successful_imports = [r for r in import_results if r[\"success\"]]\n",
    "failed_imports = [r for r in import_results if not r[\"success\"]]\n",
    "\n",
    "print(f\"Total images: {len(image_paths)}\")\n",
    "print(f\"Successfully imported: {len(successful_imports)}\")\n",
    "print(f\"Failed imports: {len(failed_imports)}\")\n",
    "\n",
    "if failed_imports:\n",
    "    print(\"\\nFailed imports:\")\n",
    "    for fail in failed_imports:\n",
    "        print(f\"  {fail['image_path']}: {fail['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run thumbnail generation\n",
    "\n",
    "This endpoint will trigger a worker to generate thumbnails for the new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/import/update_thumbnails\",\n",
    "    headers=headers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'message': 'Thumbnail update task queued successfully',\n",
       " 'task_id': '53d70cd9-fb38-4b6f-8011-3419a2da17fa',\n",
       " 'error': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viewer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
